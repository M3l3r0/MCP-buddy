<div align="center">

# ğŸ¤– MCPbuddy

### *Your intelligent companion for MCP servers with AI-powered enhancements*

[![Node.js](https://img.shields.io/badge/Node.js-18+-339933?style=for-the-badge&logo=node.js&logoColor=white)](https://nodejs.org/)
[![React](https://img.shields.io/badge/React-18.2-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://react.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-3178C6?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](LICENSE)

**A modern, elegant interface to manage and interact with MCP (Model Context Protocol) servers**

[ğŸš€ Quick Start](#-quick-start) â€¢ [âš™ï¸ Configuration](#ï¸-configuration) â€¢ [âœ¨ Features](#-features) â€¢ [ğŸ“š Documentation](#-documentation)

</div>

---

## ğŸ“¸ Preview

<div align="center">
  <img src=".github/assets/mcpbuddy-screenshot.png" alt="MCPbuddy Interface" width="800"/>
  <p><i>Modern chat interface with MCP server management and AI enhancement</i></p>
</div>

---

## ğŸ“‹ Table of Contents

- [ğŸš€ Quick Start](#-quick-start)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸ“„ License](#-license)

---

## ğŸš€ Quick Start

Get MCPbuddy up and running in less than 5 minutes!

### Prerequisites

Before you begin, ensure you have:

- ğŸŸ¢ **Node.js 18+** installed ([Download](https://nodejs.org/))
- ğŸ“¦ **npm** (comes with Node.js)
- ğŸ”Œ An MCP server endpoint (e.g., Snowflake MCP Server)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/M3l3r0/MCP-buddy.git
cd MCP-buddy

# 2. Install dependencies
npm install

# 3. Start the application
npm run dev
```

### Access the Application

Once started, open your browser:

- ğŸŒ **Frontend**: http://localhost:3000
- ğŸ”§ **Backend API**: http://localhost:3001

That's it! You're ready to configure your first MCP server. ğŸ‰

---

## âš™ï¸ Configuration

### ğŸ”Œ Step 1: Add Your First MCP Server

**Option A: Using the UI (Recommended)**

1. Click **"âš™ï¸ Servers"** in the header
2. Click **"+ Add Server"**
3. Fill in your server details:

**Example: Snowflake MCP Server**
```
Name: My Snowflake Server
Server URL: https://your-account.snowflakecomputing.com/api/v2/databases/DB/schemas/SCHEMA/mcp-servers/your_server
Auth Method: Bearer Token
Bearer Token: [your-jwt-token]
```

**Example: API Key Authentication**
```
Name: My Custom API
Server URL: https://api.example.com/mcp
Auth Method: API Key
API Key: [your-api-key]
Header Name: X-API-Key
```

4. Click **"Add Server"**
5. Your configuration is automatically saved in browser localStorage

**Option B: Configuration File (For Developers)**

```bash
# Create a local config file
cp config.local.json.example public/config.local.json

# Edit with your credentials (this file is git-ignored)
```

> ğŸ“– See complete examples in [CONFIG_EXAMPLE.md](docs/CONFIG_EXAMPLE.md)

---

### ğŸ¤– Step 2: Configure AI Enhancement (Optional)

Enhance raw MCP responses with natural language using LLMs.

1. Click **"ğŸ¤– LLM Config"** in the header
2. Click **"+ Add LLM Config"**
3. Choose your provider and configure:

**Example: Snowflake Cortex**
```
Name: Snowflake Claude
Provider: â„ï¸ Snowflake Cortex
Model: claude-3-5-sonnet
API Key: [your-snowflake-jwt-token]
Endpoint: https://your-account.snowflakecomputing.com/api/v2/cortex/inference:complete
Temperature: 0.7
Max Tokens: 2000
```

**Example: OpenAI**
```
Name: GPT-4
Provider: ğŸ¤– OpenAI
Model: gpt-4
API Key: sk-...
Endpoint: https://api.openai.com/v1/chat/completions
```

**Example: Local LLM (Ollama)**
```
Name: Local Llama
Provider: ğŸ¦™ Ollama
Model: llama3
Endpoint: http://localhost:11434/api/chat
```

4. Click **"Add Config"**
5. Toggle the AI button in the header to **"AI: ON"** (green)

> ğŸ“– More examples: [CONFIG_EXAMPLE.md](docs/CONFIG_EXAMPLE.md) | [LOCAL_CONFIG.md](docs/LOCAL_CONFIG.md)

---

### ğŸ”’ Security Best Practices

> âš ï¸ **IMPORTANT**: MCPbuddy does NOT include hardcoded credentials.

- âœ… All configurations are stored in **browser localStorage**
- âœ… **NO** credentials are committed to the repository
- âœ… Sensitive files are git-ignored (`.env`, `*credentials*`, `*.key`, `*-local.json`)
- âœ… Each user configures their own endpoints and tokens

**Never share your tokens, API keys, or endpoints in issues or PRs!** ğŸ”

See [SECURITY.md](docs/SECURITY.md) for more information.

---

## âœ¨ Features

### ğŸ¯ Core Capabilities

| Feature | Description |
|---------|-------------|
| ğŸ’¬ **Interactive Chat** | Modern messenger-like interface with message bubbles and auto-scroll |
| ğŸ”Œ **Multi-Server MCP** | Connect and manage multiple MCP servers simultaneously |
| ğŸ¤– **AI Enhancement** | Transform raw MCP responses into natural language with LLMs |
| ğŸ”„ **JSON-RPC 2.0** | Standard protocol communication with MCP servers |
| ğŸ’¾ **Local Persistence** | All configurations saved securely in browser storage |

### ğŸ§  LLM Support

MCPbuddy works with all major LLM providers:

| Provider | Models | Status |
|----------|--------|--------|
| â„ï¸ **Snowflake Cortex** | llama3-70b, llama3.1-405b, mistral-large2, mixtral-8x7b, claude-3-5-sonnet | âœ… Supported |
| ğŸ¤– **OpenAI** | GPT-4, GPT-4-turbo, GPT-3.5-turbo | âœ… Supported |
| ğŸ§  **Anthropic** | Claude 3 Opus, Claude 3 Sonnet, Claude 3 Haiku | âœ… Supported |
| ğŸ¦™ **Ollama** | llama3, mistral, codellama, and more | âœ… Supported |
| ğŸ”® **LM Studio** | Any local model | âœ… Supported |
| âš™ï¸ **Custom APIs** | OpenAI-compatible endpoints | âœ… Supported |

### ğŸ¨ Modern UI/UX

- ğŸŒ™ **Dark Theme** - Elegant design with gradients and glassmorphism effects
- ğŸ“± **Fully Responsive** - Works seamlessly on desktop, tablet, and mobile
- âš¡ **Hot Reload** - Fast development with Vite HMR
- ğŸ­ **Smooth Animations** - Fluid transitions and visual feedback
- ğŸ”” **Live Status Indicators** - Loading states, errors, and connection status
- ğŸ“Š **Real-time Logs Panel** - Monitor MCP messages and response times
- â±ï¸ **Performance Metrics** - Detailed timing breakdown (MCP, LLM, Total)
- ğŸ” **Technical Details** - Inspect full requests/responses in messages

### ğŸ” Authentication Methods

- ğŸ« **Bearer Token** - JWT or OAuth tokens
- ğŸ”‘ **API Key** - Custom header authentication
- ğŸ‘¤ **Basic Auth** - Username/password
- ğŸ”“ **OAuth 2.0** - Full OAuth flow
- âš™ï¸ **Custom Auth** - Flexible custom headers

---

## ğŸ“– How to Use

### 1ï¸âƒ£ Select an MCP Server

- Servers appear in the left panel
- Click on a server to activate it
- Active server shows green indicator ğŸŸ¢ in header

### 2ï¸âƒ£ Toggle AI Enhancement

- **AI: ON** (green) - Responses enhanced by LLM with natural language
- **AI: OFF** (gray) - Raw responses directly from MCP server

### 3ï¸âƒ£ Start Chatting

1. Type your question in the text field
2. Press **Enter** or click send â¤
3. The system will:
   - ğŸ“¡ Query the MCP Server using JSON-RPC 2.0
   - ğŸ”§ List available tools (`tools/list`)
   - ğŸ¯ Execute the appropriate tool (`tools/call`)
   - ğŸ¤– (If AI is ON) Enhance response with LLM
   - ğŸ’¬ Display the answer in chat

### 4ï¸âƒ£ Manage Conversations

- **Clear Chat** - Reset current conversation history
- **Conversations Sidebar** - Browse past chats
- Context maintained per server (up to 20 messages)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend  â”‚  (Port 3000)
â”‚   TypeScript     â”‚  Chat UI, Server Manager, Config
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/JSON
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Express Server  â”‚  (Port 3001)
â”‚     Node.js      â”‚  Proxy, Request Handler
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP    â”‚ â”‚   LLM    â”‚
â”‚  Server  â”‚ â”‚ Provider â”‚
â”‚ (Queries)â”‚ â”‚(Enhance) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow (with AI Enhancement)

```mermaid
graph LR
    A[User] -->|Query| B[Frontend]
    B -->|HTTP Request| C[Backend]
    C -->|JSON-RPC 2.0| D[MCP Server]
    D -->|Raw Response| C
    C -->|Process| E[LLM API]
    E -->|Enhanced Response| C
    C -->|Final Response| B
    B -->|Display| A
    
    style A fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#9C27B0,stroke:#333,stroke-width:2px,color:#fff
    style E fill:#E91E63,stroke:#333,stroke-width:2px,color:#fff
```

**Key Components:**

- **Frontend (React + TypeScript)**: Chat interface, server management, LLM configuration
- **Backend (Express)**: Proxy server, handles CORS, processes MCP/LLM requests
- **MCP Server**: Executes queries via JSON-RPC 2.0 protocol
- **LLM Provider**: Enhances responses with natural language processing

---

## ğŸ› ï¸ Tech Stack

### Frontend

| Technology | Purpose |
|-----------|---------|
| âš›ï¸ React 18 | UI framework |
| ğŸ“˜ TypeScript | Type safety |
| ğŸ¨ Tailwind CSS | Utility-first styling |
| âš¡ Vite | Build tool & dev server |
| ğŸ”Œ Axios | HTTP client |

### Backend

| Technology | Purpose |
|-----------|---------|
| ğŸŸ¢ Node.js 18+ | Runtime environment |
| ğŸš‚ Express | Web framework |
| ğŸ”Œ Axios | HTTP client |
| ğŸ”„ JSON-RPC 2.0 | MCP protocol |

### Additional Features

- ğŸ’¾ **LocalStorage API** - Configuration persistence
- ğŸ” **Multiple Auth Methods** - Bearer, API Key, OAuth, Basic, Custom
- ğŸŒŠ **Server-Sent Events (SSE)** - LLM response streaming
- ğŸ­ **Glassmorphism Design** - Modern UI aesthetic

---

## ğŸ“ Project Structure

```
mcp-buddy/
â”œâ”€â”€ ğŸ“‚ src/                      # React frontend
â”‚   â”œâ”€â”€ ğŸ“‚ components/           # React components
â”‚   â”‚   â”œâ”€â”€ Chat.tsx            # Main chat interface
â”‚   â”‚   â”œâ”€â”€ Header.tsx          # Top navigation bar
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx   # Chat message bubbles
â”‚   â”‚   â”œâ”€â”€ ServerManager.tsx   # MCP server management
â”‚   â”‚   â”œâ”€â”€ LLMConfigManager.tsx # LLM configuration
â”‚   â”‚   â”œâ”€â”€ ConversationsSidebar.tsx # Chat history
â”‚   â”‚   â””â”€â”€ LogsPanel.tsx       # Real-time logs
â”‚   â”œâ”€â”€ types.ts                # TypeScript types
â”‚   â”œâ”€â”€ App.tsx                 # Root component
â”‚   â”œâ”€â”€ main.tsx                # Entry point
â”‚   â””â”€â”€ index.css               # Global styles
â”‚
â”œâ”€â”€ ğŸ“‚ server/                   # Express backend
â”‚   â””â”€â”€ index.js                # API server
â”‚
â”œâ”€â”€ ğŸ“‚ public/                   # Static assets
â”‚   â””â”€â”€ config.local.json       # Local config (git-ignored)
â”‚
â”œâ”€â”€ ğŸ“„ package.json             # Dependencies
â”œâ”€â”€ ğŸ“„ vite.config.js           # Vite configuration
â”œâ”€â”€ ğŸ“„ tailwind.config.js       # Tailwind setup
â”œâ”€â”€ ğŸ“„ tsconfig.json            # TypeScript config
â””â”€â”€ ğŸ“„ README.md                # This file
```

---

## ğŸ”§ Development

### Available Scripts

```bash
# Run both frontend and backend concurrently
npm run dev

# Run frontend only (Vite dev server)
npm run dev:client

# Run backend only (Express server)
npm run dev:server

# Build for production
npm run build

# Preview production build
npm run preview
```

### Environment Variables (Optional)

Create a `.env` file in the project root:

```env
# Frontend port
VITE_PORT=3000

# Backend port
SERVER_PORT=3001
```

### Development Tips

- ğŸ”¥ Hot reload enabled for fast development
- ğŸ§ª Test with local LLMs using Ollama for free
- ğŸ“Š Use the Logs Panel to debug MCP/LLM communication
- ğŸ” Click "View Details" on messages to inspect full request/response

---

## ğŸ› Troubleshooting

<details>
<summary><b>âŒ Server not responding (404/405 error)</b></summary>

**Solutions:**
- Verify server URL is correct
- Check that Bearer token/API key is valid and not expired
- Ensure MCP server is active and accessible
- Check server logs for specific error messages
</details>

<details>
<summary><b>âŒ LLM not enhancing responses</b></summary>

**Solutions:**
1. Verify **"AI: ON"** button is green in header
2. Check that you have an LLM configured in "LLM Config"
3. Review backend terminal for error logs
4. Verify LLM token/API key is valid
5. Test LLM endpoint independently (e.g., with curl)
</details>

<details>
<summary><b>âŒ CORS errors in browser</b></summary>

**Solutions:**
- Ensure both frontend (port 3000) and backend (port 3001) are running
- Backend acts as proxy to prevent CORS issues
- Check proxy configuration in `vite.config.js`
- Restart both servers with `npm run dev`
</details>

<details>
<summary><b>âŒ Garbled LLM responses</b></summary>

**Solutions:**
- This issue is fixed in the latest version
- Backend automatically parses SSE format from Snowflake Cortex
- Update to the latest code from the repository
</details>

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Ignacio Melero**  
ğŸ“§ ignacio.melero@snowflake.com  
ğŸ™ GitHub: [@M3l3r0](https://github.com/M3l3r0)

---

## ğŸ™ Acknowledgments

Special thanks to:

- ğŸ§  [Anthropic](https://www.anthropic.com/) - For creating the MCP protocol
- â„ï¸ [Snowflake](https://www.snowflake.com/) - For Snowflake Cortex AI
- âš¡ [Vite Team](https://vitejs.dev/) - For the blazing fast build tool
- âš›ï¸ [React Team](https://react.dev/) - For the amazing UI framework
- ğŸŒŸ The open source community

---

## ğŸ“š Documentation

### Additional Resources

- ğŸ“– [Model Context Protocol Docs](https://modelcontextprotocol.io/)
- â„ï¸ [Snowflake Cortex REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-rest-api)
- ğŸ¤– [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- ğŸ§  [Anthropic API Reference](https://docs.anthropic.com/claude/reference)
- ğŸ¦™ [Ollama Documentation](https://ollama.ai/docs)

### Project Documentation

- ğŸ“‹ [Configuration Examples](docs/CONFIG_EXAMPLE.md)
- ğŸ”§ [Local Development Setup](docs/LOCAL_CONFIG.md)
- ğŸš€ [Quick Start Guide](docs/QUICKSTART.md)
- ğŸ“ [Changelog](CHANGELOG.md)
- ğŸ”’ [Security Policy](docs/SECURITY.md)

---

<div align="center">

### â­ Star this project if you find it useful! â­

[![GitHub stars](https://img.shields.io/github/stars/M3l3r0/MCP-buddy?style=social)](https://github.com/M3l3r0/MCP-buddy/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/M3l3r0/MCP-buddy?style=social)](https://github.com/M3l3r0/MCP-buddy/network/members)

[ğŸ› Report Bug](https://github.com/M3l3r0/MCP-buddy/issues) â€¢ [âœ¨ Request Feature](https://github.com/M3l3r0/MCP-buddy/issues) â€¢ [ğŸ“– Documentation](https://github.com/M3l3r0/MCP-buddy/wiki)

---

Made with â¤ï¸ and â˜• by the MCPbuddy team

</div>
